<!DOCTYPE html>
<!-- saved from url=(0074)https://cdpn.io/mediapipe-preview/fullpage/OJBVQJm?anon=true&view=fullpage -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>MediaPipe Face Landmarker Task for web</title>

    
<style>


@use "@material";
body {
  font-family: helvetica, arial, sans-serif;
  margin: 2em;
  color: #3d3d3d;
  --mdc-theme-primary: #007f8b;
  --mdc-theme-on-primary: #f1f3f4;
}

h1 {
  font-style: italic;
  color: #ff6f00;
  color: #007f8b;
}

h2 {
  clear: both;
}

em {
  font-weight: bold;
}

video {
  clear: both;
  display: block;
  transform: rotateY(180deg);
  -webkit-transform: rotateY(180deg);
  -moz-transform: rotateY(180deg);
}

section {
  opacity: 1;
  transition: opacity 500ms ease-in-out;
}

header,
footer {
  clear: both;
}

.removed {
  display: none;
}

.invisible {
  opacity: 0.2;
}

.note {
  font-style: italic;
  font-size: 130%;
}

.videoView,
.detectOnClick,
.blend-shapes {
  position: relative;
  float: left;
  width: 48%;
  margin: 2% 1%;
  cursor: pointer;
}

.videoView p,
.detectOnClick p {
  position: absolute;
  padding: 5px;
  background-color: #007f8b;
  color: #fff;
  border: 1px dashed rgba(255, 255, 255, 0.7);
  z-index: 2;
  font-size: 12px;
  margin: 0;
}

.highlighter {
  background: rgba(0, 255, 0, 0.25);
  border: 1px dashed #fff;
  z-index: 1;
  position: absolute;
}

.canvas {
  z-index: 1;
  position: absolute;
  pointer-events: none;
}

.output_canvas {
  transform: rotateY(180deg);
  -webkit-transform: rotateY(180deg);
  -moz-transform: rotateY(180deg);
}

.detectOnClick {
  z-index: 0;
}

.detectOnClick img {
  width: 100%;
}

.blend-shapes-item {
  display: flex;
  align-items: center;
  height: 20px;
}

.blend-shapes-label {
  display: flex;
  width: 120px;
  justify-content: flex-end;
  align-items: center;
  margin-right: 4px;
}

.blend-shapes-value {
  display: flex;
  height: 16px;
  align-items: center;
  background-color: #007f8b;
}
</style>
 
  
</head>

<body translate="no">
  <!-- Copyright 2023 The MediaPipe Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. -->


  
  <meta http-equiv="Cache-control" content="no-cache, no-store, must-revalidate">
  <meta http-equiv="Pragma" content="no-cache">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
  <title>Face Landmarker</title>

  <link href="./material-components-web.min.css" rel="stylesheet">


  <h1>Face landmark detection using the MediaPipe FaceLandmarker task</h1>

  <section id="demos" class="">
    <h2>Demo: Webcam continuous face landmarks detection</h2>
    <p>Hold your face in front of your webcam to get real-time face landmarker detection.<br>Click <b>enable webcam</b> below and grant access to the webcam if prompted.</p>

    <div id="liveView" class="videoView">
      <button id="webcamButton" class="mdc-button mdc-button--raised">ENABLE CAM</button>
      <div style="position: relative;">
        <video id="webcam" style="position: abso" autoplay="" playsinline=""></video>
        <canvas class="output_canvas" id="output_canvas" style="position: absolute; left: 0px; top: 0px;"></canvas>
      </div>
    </div>
    <div class="blend-shapes">
      <ul class="blend-shapes-list" id="video-blend-shapes"></ul>
    </div>
  </section>
<img id="outputImage"/>
<br>
<table>
  <tr>
    <th>Left</th>
    <th>Center</th>
    <th>Right</th>
  <tr>
  <tr>
    <td><img width="300px" id="outputLeftImage"/></td>
    <td><img width="300px" id="outputCenterImage"/></td>
    <td><img width="300px" id="outputRightImage"/></td>
  </tr>
</table>

  
    <script id="rendered-js" type="module">
// limitations under the License.
import vision from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";
const { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;
const demosSection = document.getElementById("demos");
const imageBlendShapes = document.getElementById("image-blend-shapes");
const videoBlendShapes = document.getElementById("video-blend-shapes");
let faceLandmarker;
let runningMode = "IMAGE";
let enableWebcamButton;
let webcamRunning = false;
const videoWidth = 480;
// Before we can use HandLandmarker class we must wait for it to finish
// loading. Machine Learning models can be large and take a moment to
// get everything needed to run.
async function createFaceLandmarker() {
    const filesetResolver = await FilesetResolver.forVisionTasks("https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm");
    faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
        baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
            delegate: "GPU"
        },
        outputFaceBlendshapes: false,
        outputFacialTransformationMatrixes: true,
        runningMode,
        numFaces: 1
    });
    demosSection.classList.remove("invisible");
}
createFaceLandmarker();

/********************************************************************
// Demo 2: Continuously grab image from webcam stream and detect it.
********************************************************************/
const video = document.getElementById("webcam");
const canvasElement = document.getElementById("output_canvas");
const canvasCtx = canvasElement.getContext("2d");
// Check if webcam access is supported.
function hasGetUserMedia() {
    return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
}
// If webcam supported, add event listener to button for when user
// wants to activate it.
if (hasGetUserMedia()) {
    enableWebcamButton = document.getElementById("webcamButton");
    enableWebcamButton.addEventListener("click", enableCam);
}
else {
    console.warn("getUserMedia() is not supported by your browser");
}
// Enable the live webcam view and start detection.
function enableCam(event) {
    if (!faceLandmarker) {
        console.log("Wait! faceLandmarker not loaded yet.");
        return;
    }
    if (webcamRunning === true) {
        webcamRunning = false;
        enableWebcamButton.innerText = "ENABLE PREDICTIONS";
    }
    else {
        webcamRunning = true;
        enableWebcamButton.innerText = "DISABLE PREDICTIONS";
    }
    // getUsermedia parameters.
    const constraints = {
        video: true
    };
    // Activate the webcam stream.
    navigator.mediaDevices.getUserMedia(constraints).then((stream) => {
        video.srcObject = stream;
        video.addEventListener("loadeddata", predictWebcam);
    });
}
let lastVideoTime = -1;
let results = undefined;
const drawingUtils = new DrawingUtils(canvasCtx);
async function predictWebcam() {
    const radio = video.videoHeight / video.videoWidth;
    const zoom = 1.5;
    video.style.width = zoom * videoWidth + "px";
    video.style.height = zoom * videoWidth * radio + "px";
    
    canvasElement.style.width = zoom * videoWidth + "px";
    canvasElement.style.height = zoom * videoWidth * radio + "px";
    canvasElement.width = zoom * video.videoWidth;
    canvasElement.height = zoom * video.videoHeight;

    // Now let's start detecting the stream.
    if (runningMode === "IMAGE") {
        runningMode = "VIDEO";
        await faceLandmarker.setOptions({ runningMode: runningMode });
    }
    let startTimeMs = performance.now();
    if (lastVideoTime !== video.currentTime) {
        lastVideoTime = video.currentTime;
        results = faceLandmarker.detectForVideo(video, startTimeMs);
    }
    
    if (results.faceLandmarks) {
        for (const landmarks of results.faceLandmarks) {
            drawingUtils.drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_TESSELATION, { color: "#C0C0C070", lineWidth: 1 });
            //drawingUtils.drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE, { color: "#FF3030" });
            //drawingUtils.drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_RIGHT_EYEBROW, { color: "#FF3030" });
            //drawingUtils.drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_LEFT_EYE, { color: "#30FF30" });
            //drawingUtils.drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_LEFT_EYEBROW, { color: "#30FF30" });
            drawingUtils.drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_FACE_OVAL, { color: "#E0E0E0" });
            //drawingUtils.drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_LIPS, { color: "#E0E0E0" });
            //drawingUtils.drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS, { color: "#FF3030" });
            //drawingUtils.drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS, { color: "#30FF30" });
        }
    }
    drawBldrawMatrixes(videoBlendShapes, results.facialTransformationMatrixes);
    // Call this function again to keep predicting when the browser is ready.
    if (webcamRunning === true) {
        window.requestAnimationFrame(predictWebcam);
    }
}
function drawBlendShapes(el, blendShapes) {
    if (!blendShapes.length) {
        return;
    }
    console.log(blendShapes[0]);
    let htmlMaker = "";
    blendShapes[0].categories.map((shape) => {
        htmlMaker += `
      <li class="blend-shapes-item">
        <span class="blend-shapes-label">${shape.displayName || shape.categoryName}</span>
        <span class="blend-shapes-value" style="width: calc(${+shape.score * 100}% - 120px)">${(+shape.score).toFixed(4)}</span>
      </li>
    `;
    });
    el.innerHTML = htmlMaker;
}

function drawBldrawMatrixes(el, facialTransformationMatrixes) {
    if (!facialTransformationMatrixes.length) {
        return;
    }
    const matrix4x4 = facialTransformationMatrixes[0].data;
    const R_flat = [
        matrix4x4[0], matrix4x4[1], matrix4x4[2],
        matrix4x4[4], matrix4x4[5], matrix4x4[6],
        matrix4x4[8], matrix4x4[9], matrix4x4[10]
    ];


    const angles_rad = matrixToEulerAngles(R_flat);

    const yaw = radiansToDegrees(angles_rad.yaw);
    const pitch = radiansToDegrees(angles_rad.pitch);
    const roll = radiansToDegrees(angles_rad.roll);

    const data = [
      {
        name: "yaw",
        value: yaw
      },
      {
        name: "pitch",
        value: pitch
      },
      {
        name: "roll",
        value: roll
      }
    ]
    let htmlMaker = "";
    data.map((shape) => {
        htmlMaker += `
      <li class="blend-shapes-item">
        <span class="blend-shapes-label">${shape.name}</span>
        <span class="">${(+shape.value).toFixed(4)}</span>
      </li>
    `;
    });
    el.innerHTML = htmlMaker;

    //
    let left = ""
    let center = ""
    let right = ""
    const outputLeftImage = document.getElementById('outputLeftImage');
    const outputCenterImage = document.getElementById('outputCenterImage');
    const outputRightImage = document.getElementById('outputRightImage');
    if(pitch !== undefined && typeof(pitch) === 'number') {
      if(center === "" && pitch > -5 && pitch < 5) {
        center = captureFrame()
        if(center) {
          outputCenterImage.src = center
        }
      }  else if (left === "" && pitch < -25) {
        left = captureFrame()
        if(left) {
          outputLeftImage.src = left
        }
        
      } else if (right === "" && pitch > 25) {
        right = captureFrame()
        if(right) {
          outputRightImage.src = right
        }
      }
    }
}

function matrixToEulerAngles(R) {
    // Les indices de la matrice 3x3 dans un tableau plat :
    // [ R00, R01, R02,
    //   R10, R11, R12,
    //   R20, R21, R22 ]
    
    // R20 est l'élément qui nous donne l'angle de Pitch.
    const R20 = R[6];
    
    // Pitch (Rotation autour de l'axe X)
    // Nous utilisons asin(-R20) pour éviter le Gimbal Lock pour la plupart des mouvements.
    let pitch = -Math.asin(R20);
    
    // Gestion de la Singularité (Gimbal Lock) :
    // Si la tête est tournée de +/- 90 degrés (Pitch), la cos(Pitch) est ~0.
    const threshold = 0.000001;
    const cosPitch = Math.cos(pitch);

    let yaw, roll;

    if (Math.abs(cosPitch) > threshold) {
        // Cas général
        // Yaw (Rotation autour de l'axe Y - Roulis de la tête)
        yaw = Math.atan2(R[7] / cosPitch, R[8] / cosPitch); // atan2(R21/c(P), R22/c(P))
        
        // Roll (Rotation autour de l'axe Z - Inclinaison latérale)
        roll = Math.atan2(R[3] / cosPitch, R[0] / cosPitch); // atan2(R10/c(P), R00/c(P))
    } else {
        // Cas de singularité (Pitch +/- 90 degrés)
        // Nous choisissons une valeur arbitraire (souvent 0) pour l'un des angles (ici Roll)
        roll = 0; 
        
        // Le Yaw est calculé comme la somme/différence des deux rotations restantes
        yaw = Math.atan2(R[1], R[4]); // atan2(R01, R11)
        
        // Ajustement du signe si Pitch est à -90 degrés (R20 = 1)
        if (R20 > 0) {
            yaw = Math.atan2(-R[1], -R[4]);
        }
    }
    
    return { yaw, pitch, roll };
}

// Fonction utilitaire pour convertir en degrés
function radiansToDegrees(radians) {
    return radians * (180 / Math.PI);
}

function captureFrame(pitch) {
    // 1. Récupérer les éléments
    const video = document.getElementById("webcam");
    const canvas = document.getElementById("output_canvas");
    const outputImage = document.getElementById('outputImage');

    if (!video || !canvas) {
        console.error("Éléments vidéo ou canvas introuvables.");
        return null;
    }

    // S'assurer que la vidéo est chargée
    if (video.readyState < 2) { 
        console.warn("La vidéo n'est pas prête.");
        return null;
    }

    // 2. Définir les dimensions du canvas
    // Il est important que le canvas ait les mêmes dimensions que la vidéo
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    // 3. Dessiner l'image actuelle de la vidéo sur le canvas
    const context = canvas.getContext('2d');
    if (context) {
        // Dessine l'image (frame) actuelle de la vidéo, du coin (0,0) jusqu'à la taille complète.
        context.drawImage(video, 0, 0, canvas.width, canvas.height);

        // 4. Extraire les données de l'image (format Data URL)
        // 'image/jpeg' ou 'image/png' sont les formats les plus courants
        const imageDataUrl = canvas.toDataURL('image/jpeg', 0.9); // 0.9 pour la qualité JPEG
        
        return imageDataUrl;
    }

    return null;
}

  </script>

</body></html>